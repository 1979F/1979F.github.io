<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Serendipper-x</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://1979F.github.io/"/>
  <updated>2020-11-22T08:40:50.312Z</updated>
  <id>http://1979F.github.io/</id>
  
  <author>
    <name>XIAO_JING</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>小白 LeetCode 242  有效的字母异位词</title>
    <link href="http://1979F.github.io/2020/11/22/%E5%B0%8F%E7%99%BD%20LeetCode%20242%20%20%E6%9C%89%E6%95%88%E7%9A%84%E5%AD%97%E6%AF%8D%E5%BC%82%E4%BD%8D%E8%AF%8D/"/>
    <id>http://1979F.github.io/2020/11/22/小白 LeetCode 242  有效的字母异位词/</id>
    <published>2020-11-22T08:39:27.000Z</published>
    <updated>2020-11-22T08:40:50.312Z</updated>
    
    <content type="html"><![CDATA[<p><strong>字母异位词</strong></p><p>字符串由相同字母组成，但允许排列顺序不同。<br>如“aaabbb” 与 “ababab” 是字母异位词，<br>而“aabb” 与 “ab” 不是字母异位词。</p><p>题目：<code>给定两个字符串 s 和 t ，编写一个函数来判断 t 是否是 s 的字母异位词。</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">示例 1:</span><br><span class="line"></span><br><span class="line">输入: s = <span class="string">"anagram"</span>, t = <span class="string">"nagaram"</span></span><br><span class="line">输出: <span class="literal">true</span></span><br></pre></td></tr></table></figure><a id="more"></a><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">示例 2:</span><br><span class="line"></span><br><span class="line">输入: s = <span class="string">"rat"</span>, t = <span class="string">"car"</span></span><br><span class="line">输出: <span class="literal">false</span></span><br></pre></td></tr></table></figure><p>思路：先判断字符串长度是否相同，若不同，直接返回false；若相同则继续判断，给两个字符串排序，若排序后的字符串完全相等，那么返回true，否则，返回false。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isAnagram</span><span class="params">(String s, String t)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> ls = s.length();</span><br><span class="line">        <span class="keyword">int</span> lt = t.length();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(ls != lt)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">char</span>[] arrayS = s.toCharArray(); <span class="comment">//把字符串转换为数组</span></span><br><span class="line">            <span class="keyword">char</span>[] arrayT = t.toCharArray();</span><br><span class="line">            Arrays.sort(arrayS); <span class="comment">//利用数组帮助类自动排序</span></span><br><span class="line">            Arrays.sort(arrayT);</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span>(Arrays.equals(arrayS, arrayT))</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201122163611486.png#pic" alt="在这里插入图片描述"></p><ul><li>字符串转为数组 <code>.toCharArray()</code></li><li>数组排序：<code>Arrays.sort(array)</code></li><li>判断数组是否相同：<code>Arrays.equals(array1, array2)</code></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;字母异位词&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;字符串由相同字母组成，但允许排列顺序不同。&lt;br&gt;如“aaabbb” 与 “ababab” 是字母异位词，&lt;br&gt;而“aabb” 与 “ab” 不是字母异位词。&lt;/p&gt;&lt;p&gt;题目：&lt;code&gt;给定两个字符串 s 和 t ，编写一个函数来判断 t 是否是 s 的字母异位词。&lt;/code&gt;&lt;/p&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;示例 1:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;输入: s = &lt;span class=&quot;string&quot;&gt;&quot;anagram&quot;&lt;/span&gt;, t = &lt;span class=&quot;string&quot;&gt;&quot;nagaram&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;输出: &lt;span class=&quot;literal&quot;&gt;true&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
    
      <category term="LeetCode" scheme="http://1979F.github.io/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>Jupyter notebook 导入和卸载 conda 虚拟环境</title>
    <link href="http://1979F.github.io/2020/11/21/Jupyter%20notebook%20%E5%AF%BC%E5%85%A5%E5%92%8C%E5%8D%B8%E8%BD%BD%20conda%20%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"/>
    <id>http://1979F.github.io/2020/11/21/Jupyter notebook 导入和卸载 conda 虚拟环境/</id>
    <published>2020-11-21T03:10:32.000Z</published>
    <updated>2020-11-22T05:45:15.040Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一、导入"><a href="#一、导入" class="headerlink" title="一、导入"></a>一、导入</h3><p>进入Anaconda Prompt，激活要使用的虚拟环境。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate xxx <span class="comment"># 你要使用的虚拟环境名称</span></span><br></pre></td></tr></table></figure><p>安装 ipykernel 插件， 建议使用 pip 进行安装， conda 安装容易失败</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install ipykernel</span><br></pre></td></tr></table></figure><p>安装完成之后，键入以下命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m ipykernel install --name (你要导入的虚拟环境名称)  --display-name (你要显示的名称)</span><br></pre></td></tr></table></figure><a id="more"></a><p>例如：<br><img src="https://img-blog.csdnimg.cn/20201121110206620.png#pic" alt="在这里插入图片描述"><br>打开 jupyter notebook， 可以看到环境已经成功导入</p><p><img src="https://img-blog.csdnimg.cn/2020112111081565.png#pic" alt="在这里插入图片描述"></p><h3 id="二、删除"><a href="#二、删除" class="headerlink" title="二、删除"></a>二、删除</h3><p>使用命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter kernelspec remove (你要删除的虚拟环境名称  注意不是显示名称)</span><br></pre></td></tr></table></figure><p>例如：<br><img src="https://img-blog.csdnimg.cn/20201121111008966.png#pic" alt="在这里插入图片描述"><br>删除成功 ！</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;一、导入&quot;&gt;&lt;a href=&quot;#一、导入&quot; class=&quot;headerlink&quot; title=&quot;一、导入&quot;&gt;&lt;/a&gt;一、导入&lt;/h3&gt;&lt;p&gt;进入Anaconda Prompt，激活要使用的虚拟环境。&lt;/p&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;conda activate xxx &lt;span class=&quot;comment&quot;&gt;# 你要使用的虚拟环境名称&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;安装 ipykernel 插件， 建议使用 pip 进行安装， conda 安装容易失败&lt;/p&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;pip install ipykernel&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;安装完成之后，键入以下命令&lt;/p&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;python -m ipykernel install --name (你要导入的虚拟环境名称)  --display-name (你要显示的名称)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
    
      <category term="深度学习" scheme="http://1979F.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>leaf 叶子(张量)</title>
    <link href="http://1979F.github.io/2020/11/20/leaf%20%E5%8F%B6%E5%AD%90(%E5%BC%A0%E9%87%8F)/"/>
    <id>http://1979F.github.io/2020/11/20/leaf 叶子(张量)/</id>
    <published>2020-11-20T13:07:34.000Z</published>
    <updated>2020-11-22T05:46:02.559Z</updated>
    
    <content type="html"><![CDATA[<p>在pytorch的tensor类中,有个<strong>is_leaf</strong>的属性,姑且把它作为叶子节点. <strong>is_leaf</strong> 为<strong>False</strong>的时候,则不是叶子节点, <strong>is_leaf</strong>为<strong>True</strong>的时候为叶子节点(或者叶张量)</p><p><strong>所以问题来了</strong>: <strong><em>leaf的作用是什么?为什么要加 leaf?</em></strong><br> 我们都知道tensor中的 requires_grad()属性，当requires_grad()为True时我们将会记录tensor的运算过程并为自动求导做准备，但是并不是每个requires_grad()设为True的值都会在backward的时候得到相应的grad，它还必须为leaf。这就说明： <strong><em><code>leaf成为了在 requires_grad()下判断是否需要保留 grad的前提条件</code></em></strong></p><a id="more"></a>  <h4><a id="_is_leaf__8"></a><em>is_leaf()</em></h4> <ol><li>按照惯例,所有requires_grad为False的张量(Tensor) 都为叶张量( leaf Tensor)</li><li>requires_grad为True的张量(Tensor),如果他们是由用户创建的,则它们是叶张量(leaf Tensor).这意味着它们不是运算的结果,因此gra_fn为None</li><li>只有是叶张量的tensor在反向传播时才会将本身的grad传入的backward的运算中. 如果想得到当前tensor在反向传播时的grad, 可以用retain_grad()这个属性</li></ol> <p>例子:</p> <pre class="prettyprint"><code class="prism language-python has-numbering" onclick="mdcp.signin(event)" style="position: unset;"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> a<span class="token punctuation">.</span>is_leaf<span class="token boolean">True</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> b <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> b<span class="token punctuation">.</span>is_leaf<span class="token boolean">False</span><span class="token comment"># b was created by the operation that cast a cpu Tensor into a cuda Tensor</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> c <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">2</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> c<span class="token punctuation">.</span>is_leaf<span class="token boolean">False</span><span class="token comment"># c was created by the addition operation</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> d <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> d<span class="token punctuation">.</span>is_leaf<span class="token boolean">True</span><span class="token comment"># d does not require gradients and so has no operation creating it (that is tracked by the autograd engine)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> e <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> e<span class="token punctuation">.</span>is_leaf<span class="token boolean">True</span><span class="token comment"># e requires gradients and has no operations creating it</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> f <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">"cuda"</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> f<span class="token punctuation">.</span>is_leaf<span class="token boolean">True</span><span class="token comment"># f requires grad, has no operation creating it</span></code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在pytorch的tensor类中,有个&lt;strong&gt;is_leaf&lt;/strong&gt;的属性,姑且把它作为叶子节点. &lt;strong&gt;is_leaf&lt;/strong&gt; 为&lt;strong&gt;False&lt;/strong&gt;的时候,则不是叶子节点, &lt;strong&gt;is_leaf&lt;/strong&gt;为&lt;strong&gt;True&lt;/strong&gt;的时候为叶子节点(或者叶张量)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;所以问题来了&lt;/strong&gt;: &lt;strong&gt;&lt;em&gt;leaf的作用是什么?为什么要加 leaf?&lt;/em&gt;&lt;/strong&gt;&lt;br&gt; 我们都知道tensor中的 requires_grad()属性，当requires_grad()为True时我们将会记录tensor的运算过程并为自动求导做准备，但是并不是每个requires_grad()设为True的值都会在backward的时候得到相应的grad，它还必须为leaf。这就说明： &lt;strong&gt;&lt;em&gt;&lt;code&gt;leaf成为了在 requires_grad()下判断是否需要保留 grad的前提条件&lt;/code&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="深度学习" scheme="http://1979F.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>《动手学深度学习 PyTorch版》学习笔记（三）：线性回归</title>
    <link href="http://1979F.github.io/2020/10/09/%E3%80%8A%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20PyTorch%E7%89%88%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>http://1979F.github.io/2020/10/09/《动手学深度学习 PyTorch版》学习笔记（三）：线性回归/</id>
    <published>2020-10-09T06:59:41.000Z</published>
    <updated>2020-11-22T05:41:11.015Z</updated>
    
    <content type="html"><![CDATA[<p>线性回归输出是一个连续值，因此适用于回归问题。回归问题在实际中很常见，如预测房屋价格、气温、销售额等连续值的问题。</p><h3 id="一、基本要素"><a href="#一、基本要素" class="headerlink" title="一、基本要素"></a>一、基本要素</h3><p><code>摘自原书</code></p><p><strong>模型定义</strong></p><p>设房屋的面积为 x1，房龄为 x2，售出价格为 y。我们需要建立基于输入 x1 和 x2 来计算输出 y 的表达式，也就是 <strong><code>模型（model）</code></strong> 。顾名思义，线性回归假设输出与各个输入之间是线性关系：<br><img src="https://img-blog.csdnimg.cn/20201009100323806.png#pic_center" alt="在这里插入图片描述"><br>其中 w1 和 w2 是 <strong><code>权重（weight）</code></strong>，b 是 <strong><code>偏差（bias）</code></strong>，且均为标量。它们是线性回归模型的  参数（parameter）。模型输出 yˆ  是线性回归对真实价格 y 的预测或估计。我们通常允许它们之间有一定误差。</p><a id="more"></a><p><strong>模型训练</strong></p><p>我们需要通过数据来寻找特定的模型参数值，使模型在数据上的误差尽可能小。这个过程叫作 <strong><code>模型训练（model training）</code></strong>，包含三个要素：</p><ul><li>训练数据</li></ul><p>我们通常收集一系列的真实数据，例如多栋房屋的真实售出价格和它们对应的面积和房龄。我们希望在这个数据上面寻找模型参数来使模型的预测价格与真实价格的误差最小。在机器学习术语里，该数据集被称为 <strong><code>训练数据集（training data set）</code></strong> 或 <strong><code>训练集（training set）</code></strong> ，一栋房屋被称为一个  <strong><code>样本（sample）</code></strong>  ，其真实售出价格叫作 <strong><code>标签（label）</code></strong> ，用来预测标签的两个因素叫作  <strong><code>特征（feature）</code></strong>。特征用来表征样本的特点。<br><img src="https://img-blog.csdnimg.cn/20201009101536359.png#pic_center" alt="在这里插入图片描述"></p><ul><li>损失函数</li></ul><p>在模型训练中，我们需要衡量价格预测值与真实值之间的误差。通常我们会选取一个非负数作为误差，且数值越小表示误差越小。一个常用的选择是平方函数。它在评估索引为 i 的样本误差的表达式为：<br><img src="https://img-blog.csdnimg.cn/20201009101639483.png#pic_center" alt="在这里插入图片描述"><br>其中常数 1/2 使对平方项求导后的常数系数为1，这样在形式上稍微简单一些。显然，误差越小表示预测价格与真实价格越相近，且当二者相等时误差为0。给定训练数据集，这个误差只与模型参数相关，因此我们将它记为以模型参数为参数的函数。在机器学习里，将衡量误差的函数称为 <strong><code>损失函数（loss function）</code></strong> 。这里使用的平方误差函数也称为 <strong><code>平方损失（square loss）</code></strong> 。<br>通常，我们用训练数据集中所有样本误差的平均来衡量模型预测的质量，即<br><img src="https://img-blog.csdnimg.cn/20201009101817831.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RzdW5kZXJlX3g=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><ul><li>优化算法</li></ul><p>当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作 <strong><code>解析解（analytical solution）</code></strong> 。本节使用的线性回归和平方误差刚好属于这个范畴。然而，大多数深度学习模型并没有解析解，<strong>只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值</strong>。这类解叫作 <strong><code>数值解（numerical solution）</code></strong>。</p><p>在求数值解的优化算法中，<strong>小批量随机梯度下降（mini-batch stochastic gradient descent）</strong> 在深度学习中被广泛使用。它的算法很简单：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）B，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。</p><p>在训练本节讨论的线性回归模型的过程中，模型的每个参数将作如下迭代：<br><img src="https://img-blog.csdnimg.cn/20201009103857374.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RzdW5kZXJlX3g=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p><img src="https://img-blog.csdnimg.cn/20201009103924921.png#pic_center" alt="在这里插入图片描述"></p><ul><li>模型预测</li></ul><p><img src="https://img-blog.csdnimg.cn/20201009105609277.png#pic_center" alt="在这里插入图片描述"></p><h3 id="二、线性回归的表示方法"><a href="#二、线性回归的表示方法" class="headerlink" title="二、线性回归的表示方法"></a>二、线性回归的表示方法</h3><p>如果我们对训练数据集里的3个房屋样本（索引分别为1、2和3）逐一预测价格<br><img src="https://img-blog.csdnimg.cn/20201009124246381.png#pic_center" alt="在这里插入图片描述"><br>现在，我们将上面3个等式转化成矢量计算。设：</p><p><img src="https://img-blog.csdnimg.cn/2020100912431818.png#pic_center" alt="在这里插入图片描述"><br>对3个房屋样本预测价格的矢量计算表达式为yˆ=Xw+b, 其中的加法运算使用了 <code>广播机制</code></p><p> <img src="https://img-blog.csdnimg.cn/20201009145117634.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RzdW5kZXJlX3g=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;线性回归输出是一个连续值，因此适用于回归问题。回归问题在实际中很常见，如预测房屋价格、气温、销售额等连续值的问题。&lt;/p&gt;&lt;h3 id=&quot;一、基本要素&quot;&gt;&lt;a href=&quot;#一、基本要素&quot; class=&quot;headerlink&quot; title=&quot;一、基本要素&quot;&gt;&lt;/a&gt;一、基本要素&lt;/h3&gt;&lt;p&gt;&lt;code&gt;摘自原书&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;模型定义&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;设房屋的面积为 x1，房龄为 x2，售出价格为 y。我们需要建立基于输入 x1 和 x2 来计算输出 y 的表达式，也就是 &lt;strong&gt;&lt;code&gt;模型（model）&lt;/code&gt;&lt;/strong&gt; 。顾名思义，线性回归假设输出与各个输入之间是线性关系：&lt;br&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20201009100323806.png#pic_center&quot; alt=&quot;在这里插入图片描述&quot;&gt;&lt;br&gt;其中 w1 和 w2 是 &lt;strong&gt;&lt;code&gt;权重（weight）&lt;/code&gt;&lt;/strong&gt;，b 是 &lt;strong&gt;&lt;code&gt;偏差（bias）&lt;/code&gt;&lt;/strong&gt;，且均为标量。它们是线性回归模型的  参数（parameter）。模型输出 yˆ  是线性回归对真实价格 y 的预测或估计。我们通常允许它们之间有一定误差。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="深度学习" scheme="http://1979F.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>《动手学深度学习 PyTorch版》学习笔记（二）：自动求梯度</title>
    <link href="http://1979F.github.io/2020/10/08/%E3%80%8A%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20PyTorch%E7%89%88%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E8%87%AA%E5%8A%A8%E6%B1%82%E6%A2%AF%E5%BA%A6/"/>
    <id>http://1979F.github.io/2020/10/08/《动手学深度学习 PyTorch版》学习笔记（二）：自动求梯度/</id>
    <published>2020-10-08T15:28:53.000Z</published>
    <updated>2020-11-22T05:41:50.926Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、自动求梯度"><a href="#一、自动求梯度" class="headerlink" title="一、自动求梯度"></a>一、自动求梯度</h2><p><strong>1、requires_grad_(), detach(), torch.no_grad()的区别</strong><br>参考博客：<a href="https://www.jianshu.com/p/ff74ccae25f3" target="_blank" rel="noopener">https://www.jianshu.com/p/ff74ccae25f3</a></p><p><strong>2、.grad_fn</strong><br>每个Tensor都有一个.grad_fn属性，该属性即创建该Tensor的Function, 就是说该Tensor是不是通过某些运算得到的，若是，则grad_fn返回一个与这些运算相关的对象，否则是None。</p><a id="more"></a><p><strong>3、梯度</strong></p><ul><li>grad在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零 <code>.grad.data.zero_()</code></li><li>在y.backward()时，如果y是标量，则不需要为backward()传入任何参数；否则，需要传入一个与y同形的Tensor</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = <span class="number">2</span> * x</span><br><span class="line">z = y.view(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">print(z)</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">2.</span>, <span class="number">4.</span>],</span><br><span class="line">        [<span class="number">6.</span>, <span class="number">8.</span>]], grad_fn=&lt;ViewBackward&gt;)</span><br></pre></td></tr></table></figure><p>现在 z 不是一个标量，所以在调用backward时需要传入一个和z同形的权重向量进行加权求和得到一个标量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">v = torch.tensor([[<span class="number">1.0</span>, <span class="number">0.1</span>], [<span class="number">0.01</span>, <span class="number">0.001</span>]], dtype=torch.float)</span><br><span class="line">z.backward(v)</span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">2.0000</span>, <span class="number">0.2000</span>, <span class="number">0.0200</span>, <span class="number">0.0020</span>])</span><br></pre></td></tr></table></figure><p><strong>4、中断梯度</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">1.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y1 = x ** <span class="number">2</span> </span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    y2 = x ** <span class="number">3</span></span><br><span class="line">y3 = y1 + y2</span><br><span class="line"></span><br><span class="line">print(x.requires_grad)</span><br><span class="line">print(y1, y1.requires_grad) <span class="comment"># True</span></span><br><span class="line">print(y2, y2.requires_grad) <span class="comment"># False</span></span><br><span class="line">print(y3, y3.requires_grad) <span class="comment"># True</span></span><br><span class="line">y3.backward()</span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201008232836301.png#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、自动求梯度&quot;&gt;&lt;a href=&quot;#一、自动求梯度&quot; class=&quot;headerlink&quot; title=&quot;一、自动求梯度&quot;&gt;&lt;/a&gt;一、自动求梯度&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;1、requires_grad_(), detach(), torch.no_grad()的区别&lt;/strong&gt;&lt;br&gt;参考博客：&lt;a href=&quot;https://www.jianshu.com/p/ff74ccae25f3&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.jianshu.com/p/ff74ccae25f3&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2、.grad_fn&lt;/strong&gt;&lt;br&gt;每个Tensor都有一个.grad_fn属性，该属性即创建该Tensor的Function, 就是说该Tensor是不是通过某些运算得到的，若是，则grad_fn返回一个与这些运算相关的对象，否则是None。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="深度学习" scheme="http://1979F.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>《动手学深度学习 PyTorch版》学习笔记（一）：数据操作</title>
    <link href="http://1979F.github.io/2020/10/08/%E3%80%8A%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20PyTorch%E7%89%88%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C/"/>
    <id>http://1979F.github.io/2020/10/08/《动手学深度学习 PyTorch版》学习笔记（一）：数据操作/</id>
    <published>2020-10-08T14:57:32.000Z</published>
    <updated>2020-11-22T05:42:22.786Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、数据操作"><a href="#一、数据操作" class="headerlink" title="一、数据操作"></a>一、数据操作</h2><p>在PyTorch中，torch.Tensor是存储和变换数据的主要工具。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&quot;tensor&quot;这个单词一般可译作“张量”，张量可以看作是一个多维数组。</span><br><span class="line">标量可以看作是0维张量，向量可以看作1维张量，矩阵可以看作是二维张量。</span><br></pre></td></tr></table></figure><p><strong>1、torch.arange() 和torch.linspace</strong></p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># arange(s, e, step) =&gt; 从s到e，步长为step</span></span><br><span class="line">x8 = torch.arange(<span class="number">1</span>, <span class="number">10</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># print(x8)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># linspace(s, e, steps) =&gt; 从s到e，均匀切分成steps份</span></span><br><span class="line">x9 = torch.linspace(<span class="number">2</span>,<span class="number">8</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment"># print(x9)</span></span><br></pre></td></tr></table></figure><p><strong>2、torch.range() 和torch.arange() 的区别</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>y=torch.range(<span class="number">1</span>,<span class="number">6</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.dtype</span><br><span class="line">torch.float32</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z=torch.arange(<span class="number">1</span>,<span class="number">6</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z.dtype</span><br><span class="line">torch.int64</span><br></pre></td></tr></table></figure><p><strong>3、torch.randn与torch.rand的区别</strong></p><p><strong>randn</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.randn(*sizes, out=<span class="literal">None</span>) → Tensor</span><br></pre></td></tr></table></figure><p>返回一个包含了从<code>标准正态分布</code>中抽取的一组随机数的张量</p><p><code>size</code>：张量的形状</p><p><code>out</code>：结果张量</p><p><strong>rand</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.rand(*sizes, out=<span class="literal">None</span>) → Tensor</span><br></pre></td></tr></table></figure><p>返回一个张量，包含了从区间[0, 1)的<code>均匀分布</code>中抽取的一组随机数</p><p><strong>4、NumPy数组与Tensor的互相转换</strong>（共享内存）</p><p>NumPy转Tensor：<code>torch.from_numpy()</code><br>Tensor转NumPy：<code>numpy()</code><br>另：可以使用 torch.tensor() 将NumPy数组转换成Tensor，但不再共享内存</p><p><strong>5、Tensor on GPU</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span>)          <span class="comment"># GPU</span></span><br><span class="line">    y = torch.ones_like(x, device=device)  <span class="comment"># 直接创建一个在GPU上的Tensor</span></span><br><span class="line">    x = x.to(device)                       <span class="comment"># 等价于 .to("cuda")</span></span><br><span class="line">    z = x + y</span><br><span class="line">    print(z)</span><br><span class="line">    print(z.to(<span class="string">"cpu"</span>, torch.double))       <span class="comment"># to()还可以同时更改数据类型</span></span><br></pre></td></tr></table></figure><p><strong>6、索引</strong></p><p>索引出来的结果与元数据共享内存 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">print(x)</span><br><span class="line">y = x[<span class="number">0</span>, :]  <span class="comment"># 取出第一行</span></span><br><span class="line">print(y)</span><br><span class="line">y += <span class="number">1</span></span><br><span class="line">print(y)</span><br><span class="line">print(x[<span class="number">0</span>, :]) <span class="comment"># 源tensor也被改了</span></span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">tensor([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">tensor([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure><p><strong>7、广播机制</strong></p><p>当对两个形状不同的Tensor按元素运算时，可能会触发广播（broadcasting）机制：先适当复制元素使这两个Tensor形状相同后再按元素运算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">1</span>, <span class="number">3</span>).view(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">print(x)</span><br><span class="line">y = torch.arange(<span class="number">1</span>, <span class="number">4</span>).view(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">print(y)</span><br><span class="line">print(x + y)</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>]])</span><br><span class="line">tensor([[<span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>]])</span><br><span class="line">tensor([[<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>]])</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、数据操作&quot;&gt;&lt;a href=&quot;#一、数据操作&quot; class=&quot;headerlink&quot; title=&quot;一、数据操作&quot;&gt;&lt;/a&gt;一、数据操作&lt;/h2&gt;&lt;p&gt;在PyTorch中，torch.Tensor是存储和变换数据的主要工具。&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;quot;tensor&amp;quot;这个单词一般可译作“张量”，张量可以看作是一个多维数组。&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;标量可以看作是0维张量，向量可以看作1维张量，矩阵可以看作是二维张量。&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;1、torch.arange() 和torch.linspace&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="深度学习" scheme="http://1979F.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>torch.randn与torch.rand的区别</title>
    <link href="http://1979F.github.io/2020/10/05/torch.randn%E4%B8%8Etorch.rand%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>http://1979F.github.io/2020/10/05/torch.randn与torch.rand的区别/</id>
    <published>2020-10-05T04:53:15.000Z</published>
    <updated>2020-11-22T07:01:48.362Z</updated>
    
    <content type="html"><![CDATA[<h4 id="randn"><a href="#randn" class="headerlink" title="randn"></a>randn</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.randn(*sizes, out=<span class="literal">None</span>) → Tensor</span><br></pre></td></tr></table></figure><p>返回一个包含了从标准正态分布中抽取的一组随机数的张量</p><p><code>size</code>：张量的形状</p><p><code>out</code>：结果张量</p><h4 id="rand"><a href="#rand" class="headerlink" title="rand"></a>rand</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.rand(*sizes, out=<span class="literal">None</span>) → Tensor</span><br></pre></td></tr></table></figure><p>[0,1)之间的均匀分布</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;randn&quot;&gt;&lt;a href=&quot;#randn&quot; class=&quot;headerlink&quot; title=&quot;randn&quot;&gt;&lt;/a&gt;randn&lt;/h4&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter
      
    
    </summary>
    
    
    
      <category term="深度学习" scheme="http://1979F.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 之 requires_grad，requires_grad_()，grad_fn</title>
    <link href="http://1979F.github.io/2020/10/04/PyTorch%20%E4%B9%8B%20requires_grad%EF%BC%8Crequires_grad_()%EF%BC%8Cgrad_fn/"/>
    <id>http://1979F.github.io/2020/10/04/PyTorch 之 requires_grad，requires_grad_()，grad_fn/</id>
    <published>2020-10-04T11:33:52.000Z</published>
    <updated>2020-11-22T07:07:05.540Z</updated>
    
    <content type="html"><![CDATA[<ul><li>x.grad_fn和x.requires_grad为x的属性</li><li>x.grad_fn：积分方法名，默认为None</li><li>x.requires_grad：是否积分的属性，默认为False</li><li>x.requires_grad_()：设置积分的方法，设置之后requires_grad为True</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""Tensor"""</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 创建一个Tensor并设置requires_grad=True</span></span><br><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x.grad_fn)</span><br><span class="line"></span><br><span class="line">y = x + <span class="number">2</span></span><br><span class="line">print(y)</span><br><span class="line">print(y.grad_fn)</span><br><span class="line"></span><br><span class="line">print(x.is_leaf, y.is_leaf)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;x.grad_fn和x.requires_grad为x的属性&lt;/li&gt;
&lt;li&gt;x.grad_fn：积分方法名，默认为None&lt;/li&gt;
&lt;li&gt;x.requires_grad：是否积分的属性，默认为False&lt;/li&gt;
&lt;li&gt;x.requires_grad_
      
    
    </summary>
    
    
    
      <category term="PyTorch" scheme="http://1979F.github.io/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>指定tensorflow运行的GPU或CPU设备</title>
    <link href="http://1979F.github.io/2020/07/29/%E6%8C%87%E5%AE%9Atensorflow%E8%BF%90%E8%A1%8C%E7%9A%84GPU%E6%88%96CPU%E8%AE%BE%E5%A4%87/"/>
    <id>http://1979F.github.io/2020/07/29/指定tensorflow运行的GPU或CPU设备/</id>
    <published>2020-07-29T08:11:15.000Z</published>
    <updated>2020-11-22T05:44:23.748Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>如果 TensorFlow 指令中兼有 CPU 和 GPU 实现，当该指令分配到设备时，GPU 设备有优先权。</p></li><li><p>如果你的系统里有多个 GPU, 那么 ID 最小的 GPU 会默认使用。</p></li></ul><p>当我们要指定tensorflow运行的GPU或CPU设备时，可以使用<code>tf.device()</code>命令</p><p> 首先查看可用运算设备(CPU,GPU)</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.python.client <span class="keyword">import</span> device_lib</span><br><span class="line">print(device_lib.list_local_devices())</span><br></pre></td></tr></table></figure><p>得到类似以下的输出结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[name: <span class="string">"/device:CPU:0"</span></span><br><span class="line">device_type: <span class="string">"CPU"</span></span><br><span class="line">memory_limit: <span class="number">268435456</span></span><br><span class="line">locality &#123;</span><br><span class="line">&#125;</span><br><span class="line">incarnation: <span class="number">13177083330855175469</span></span><br><span class="line">, name: <span class="string">"/device:GPU:0"</span></span><br><span class="line">device_type: <span class="string">"GPU"</span></span><br><span class="line">memory_limit: <span class="number">10968950375</span></span><br><span class="line">locality &#123;</span><br><span class="line">  bus_id: <span class="number">1</span></span><br><span class="line">&#125;</span><br><span class="line">incarnation: <span class="number">6161624703599064583</span></span><br><span class="line">physical_device_desc: <span class="string">"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:00:08.0, compute capability: 6.1"</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>name即是对应设备名称，一般来说（以各自实际情况为准，每个人的情况可能不同）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;/cpu:0&quot;：机器的 CPU。</span><br><span class="line">&quot;/device:GPU:0&quot;：机器的 GPU（如果有一个）。</span><br><span class="line">&quot;/device:GPU:1&quot;：机器的第二个 GPU（以此类推）。</span><br></pre></td></tr></table></figure><p><strong>使用命令tf.device()进行指定</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">'/device:GPU:0'</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;&lt;p&gt;如果 TensorFlow 指令中兼有 CPU 和 GPU 实现，当该指令分配到设备时，GPU 设备有优先权。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;如果你的系统里有多个 GPU, 那么 ID 最小的 GPU 会默认使用。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;当我们要指定tensorflow运行的GPU或CPU设备时，可以使用&lt;code&gt;tf.device()&lt;/code&gt;命令&lt;/p&gt;&lt;p&gt; 首先查看可用运算设备(CPU,GPU)&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="深度学习" scheme="http://1979F.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>module ‘tensorflow‘ has no attribute ‘sparse ‘</title>
    <link href="http://1979F.github.io/2020/07/29/module%20%E2%80%98tensorflow%E2%80%98%20has%20no%20attribute%20%E2%80%98sparse%20%E2%80%98/"/>
    <id>http://1979F.github.io/2020/07/29/module ‘tensorflow‘ has no attribute ‘sparse ‘/</id>
    <published>2020-07-29T07:50:49.000Z</published>
    <updated>2020-11-22T07:08:36.191Z</updated>
    
    <content type="html"><![CDATA[<p>非常奇怪的一个报错，在查找资料无果的情况下选择了重装keras（应该是版本对应出现了问题），报错解决</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;非常奇怪的一个报错，在查找资料无果的情况下选择了重装keras（应该是版本对应出现了问题），报错解决&lt;/p&gt;

      
    
    </summary>
    
    
    
      <category term="深度学习" scheme="http://1979F.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习环境搭建之Anaconda安装keras</title>
    <link href="http://1979F.github.io/2020/07/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E4%B9%8BAnaconda%E5%AE%89%E8%A3%85keras/"/>
    <id>http://1979F.github.io/2020/07/29/深度学习环境搭建之Anaconda安装keras/</id>
    <published>2020-07-29T07:43:11.000Z</published>
    <updated>2020-11-22T07:26:32.928Z</updated>
    
    <content type="html"><![CDATA[<p><strong>一、确定安装版本号</strong><br>搭环境遇到一堆坑，总结一点最重要的就是要选择好版本。<br>  这里我们要注意tensorflow与keras的版本对应关系。见下图<br>  <img src="https://img-blog.csdnimg.cn/20200729154029110.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RzdW5kZXJlX3g=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>二、使用pip进行安装</strong></p><p>这里注意首先要激活对应的环境</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install keras==版本号</span><br></pre></td></tr></table></figure><p>后续有时间再补充安装tensorflow-gpu以及pytorch的教程</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;一、确定安装版本号&lt;/strong&gt;&lt;br&gt;搭环境遇到一堆坑，总结一点最重要的就是要选择好版本。&lt;br&gt;  这里我们要注意tensorflow与keras的版本对应关系。见下图&lt;br&gt;  &lt;img src=&quot;https://img-blog.csdnimg.
      
    
    </summary>
    
    
    
      <category term="深度学习" scheme="http://1979F.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Linux下进入Anaconda Prompt以及Navigator</title>
    <link href="http://1979F.github.io/2020/07/29/Linux%E4%B8%8B%E8%BF%9B%E5%85%A5Anaconda%20Prompt%E4%BB%A5%E5%8F%8ANavigator/"/>
    <id>http://1979F.github.io/2020/07/29/Linux下进入Anaconda Prompt以及Navigator/</id>
    <published>2020-07-29T07:36:49.000Z</published>
    <updated>2020-11-22T07:10:30.672Z</updated>
    
    <content type="html"><![CDATA[<p><code>一、进入Anaconda Promp</code></p><p>打开终端，cd 进Anaconda下的bin目录</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate</span><br></pre></td></tr></table></figure><p>退出</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda deactivate</span><br></pre></td></tr></table></figure><p><code>二、进入Anaconda Navigator</code></p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">anaconda-navigator</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;code&gt;一、进入Anaconda Promp&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;打开终端，cd 进Anaconda下的bin目录&lt;/p&gt;
&lt;figure class=&quot;highlight powershell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;p
      
    
    </summary>
    
    
    
      <category term="深度学习" scheme="http://1979F.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>哔哩哔哩n倍速播放视频</title>
    <link href="http://1979F.github.io/2020/07/15/%E5%93%94%E5%93%A9%E5%93%94%E5%93%A9n%E5%80%8D%E9%80%9F%E6%92%AD%E6%94%BE%E8%A7%86%E9%A2%91/"/>
    <id>http://1979F.github.io/2020/07/15/哔哩哔哩n倍速播放视频/</id>
    <published>2020-07-15T09:05:06.000Z</published>
    <updated>2020-11-22T07:32:11.483Z</updated>
    
    <content type="html"><![CDATA[<p>在b站看一些学习视频的时候，有时候2倍速满足不了我们的需求，可以在控制台键入以下命令实现n倍速播放视频<br><img src="https://img-blog.csdnimg.cn/20200715170441296.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RzdW5kZXJlX3g=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">document</span>.querySelector(<span class="string">'video'</span>).playbackRate = <span class="number">3</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在b站看一些学习视频的时候，有时候2倍速满足不了我们的需求，可以在控制台键入以下命令实现n倍速播放视频&lt;br&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20200715170441296.png?x-oss-process=image/wa
      
    
    </summary>
    
    
    
      <category term="工具" scheme="http://1979F.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
  <entry>
    <title>网络层的基本概念</title>
    <link href="http://1979F.github.io/2020/06/13/%E7%BD%91%E7%BB%9C%E5%B1%82%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
    <id>http://1979F.github.io/2020/06/13/网络层的基本概念/</id>
    <published>2020-06-13T08:14:56.000Z</published>
    <updated>2020-11-22T07:25:43.456Z</updated>
    
    <content type="html"><![CDATA[<h5 id="网络层的功能"><a href="#网络层的功能" class="headerlink" title="网络层的功能"></a>网络层的功能</h5><p><code>网络层负责实现各种不一样的物理网络的互联</code></p><ul><li>网络层采用分组交换技术</li><li>网络层应当使用统一编址</li><li>路由器使用存储转发实现异构网络的互联</li><li>网络层关注分组从一个网络到达另一个网络的选路问题</li></ul><h5 id="资源子网与通信子网"><a href="#资源子网与通信子网" class="headerlink" title="资源子网与通信子网"></a>资源子网与通信子网</h5><p><code>资源子网</code></p><ul><li>负责全网的数据处理业务，向网络用户提供各种网络资源与网络服务。</li><li>传输层应用层属于资源子网，关注端到端通信</li></ul><a id="more"></a><p><code>通信子网</code></p><ul><li>完成网络数据传输转发等通信处理任务</li><li>网络层、数据链路层和物理层属于通信子网，负责网络通信转发</li></ul><p><strong>分组交换网络的路由模式</strong><br><code>路由模式的概念</code></p><ul><li>分组选路的方式</li></ul><p><code>网络根据路由模式分类</code></p><ul><li>数据报网络：无连接不可靠的网络</li><li>虚电路网络：面向连接可靠的网络</li></ul><h5 id="数据报网络"><a href="#数据报网络" class="headerlink" title="数据报网络"></a>数据报网络</h5><p><code>特点：</code></p><ul><li>在网络层没有连接建立过程</li><li>路由器不维护端对端的连接状态</li><li>一般分组使用目标主机的ID（即IP地址）进行路由选择</li><li>同样的收发双方的不同分组可能经由的路径可能不同</li></ul><img src="https://img-blog.csdnimg.cn/20200613155359734.png" width="50%"><h5 id="虚电路网络"><a href="#虚电路网络" class="headerlink" title="虚电路网络"></a>虚电路网络</h5><p><code>特点：</code></p><ul><li>发送分组之前会建立一条虚拟的电路（永久PVC或临时SVC）；</li><li>每个分组携有标签（虚电路ID），由标签来确定下一跳；</li><li>在连接建立阶段确定固定的路由，全部数据通过同一条路传递；</li><li>路由器为每个正在通信中的连接维持状态</li></ul><img src="https://img-blog.csdnimg.cn/20200613155644122.png" width="50%"><h5 id="QoS的基本概念"><a href="#QoS的基本概念" class="headerlink" title="QoS的基本概念"></a>QoS的基本概念</h5><ul><li>QoS（Quality of Service）服务质量是指允许用户在带宽、延迟、抖动、可靠性（丢包率）等方面获得可预期的服务水平的一系列技术的集合。</li><li>抖动：延迟的变化程度</li></ul><h5 id="数据报网络和虚电路网络比较"><a href="#数据报网络和虚电路网络比较" class="headerlink" title="数据报网络和虚电路网络比较"></a>数据报网络和虚电路网络比较</h5><img src="https://img-blog.csdnimg.cn/20200613160811576.png" width="60%"><p><code>数据报网络</code>：无连接，可靠性不强，可能形成环路，但是速度快，路由器的工作相对小，不容易实现QoS（服务质量保证）<br><code>虚电路网络</code>：面向连接可靠性较好，路由器需要为每个虚电路维护状态，代价较高，建立虚电路需要时间，容易实现QoS</p><h5 id="网络层提供的服务"><a href="#网络层提供的服务" class="headerlink" title="网络层提供的服务"></a>网络层提供的服务</h5><ul><li><code>因特网模型</code>：无连接不可靠灵活的数据包网络传输服务</li><li><code>ATM网络</code>：面向连接可靠的虚电路传输服务</li></ul><h5 id="因特网的选择"><a href="#因特网的选择" class="headerlink" title="因特网的选择"></a>因特网的选择</h5><ul><li>网络层向上只提供简单灵活的、无连接的、尽最大努力交付的数据包服务</li><li>网络在发送分组时不需要先建立连接。每一个分组（即IP数据报）独立发送，与其前后的分组无关（不进行编号）</li><li>网络层不提供服务质量的承诺。即所传送的分组可能出错、丢失、重复和失序，当然也不保证分组传送的时限</li><li>因特网特点：灵活、适应性强、网络成本相对低</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;网络层的功能&quot;&gt;&lt;a href=&quot;#网络层的功能&quot; class=&quot;headerlink&quot; title=&quot;网络层的功能&quot;&gt;&lt;/a&gt;网络层的功能&lt;/h5&gt;&lt;p&gt;&lt;code&gt;网络层负责实现各种不一样的物理网络的互联&lt;/code&gt;&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;网络层采用分组交换技术&lt;/li&gt;
&lt;li&gt;网络层应当使用统一编址&lt;/li&gt;
&lt;li&gt;路由器使用存储转发实现异构网络的互联&lt;/li&gt;
&lt;li&gt;网络层关注分组从一个网络到达另一个网络的选路问题&lt;/li&gt;
&lt;/ul&gt;&lt;h5 id=&quot;资源子网与通信子网&quot;&gt;&lt;a href=&quot;#资源子网与通信子网&quot; class=&quot;headerlink&quot; title=&quot;资源子网与通信子网&quot;&gt;&lt;/a&gt;资源子网与通信子网&lt;/h5&gt;&lt;p&gt;&lt;code&gt;资源子网&lt;/code&gt;&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;负责全网的数据处理业务，向网络用户提供各种网络资源与网络服务。&lt;/li&gt;
&lt;li&gt;传输层应用层属于资源子网，关注端到端通信&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
    
      <category term="计算机网络" scheme="http://1979F.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>TCP的拥塞控制机制</title>
    <link href="http://1979F.github.io/2020/06/13/TCP%E7%9A%84%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6%E6%9C%BA%E5%88%B6/"/>
    <id>http://1979F.github.io/2020/06/13/TCP的拥塞控制机制/</id>
    <published>2020-06-13T03:18:19.000Z</published>
    <updated>2020-11-22T07:02:25.236Z</updated>
    
    <content type="html"><![CDATA[<p><strong>拥塞：</strong></p><ul><li>在某段时间，若对网络中某资源的需求超过了该资源所能提供的可用部分，网络的性能就要变坏，产生拥塞</li><li>若网络中有许多资源同时产生拥塞，网络的性能就要明显变坏，整个网络的吞吐量将随输入负荷的增大而下降</li></ul><p><strong>拥塞控制：</strong></p><ul><li>保证网络能够承受现有的网络负荷</li></ul><p><strong>开环控制和闭环控制</strong></p><p><code>开环控制</code></p><ul><li>在设计网络时事先将有关发生的拥塞的因素考虑周到，力求网络在工作时不产生拥塞</li></ul><a id="more"></a><p><code>闭环控制</code></p><ul><li>基于<code>反馈环路</code>的概念</li><li>监测网络系统以便检测到拥塞在何时、何处发生</li><li>将拥塞发生的信息传送到可采取行动的地方</li><li>调整网络系统的运行以解决出现的问题</li></ul><p><strong>TCP拥塞控制思想</strong><br><code>特性</code></p><ul><li>使用拥塞窗口cwnd控制发送窗口大小</li><li>发送窗口的上限值 = Min [rwnd，cwnd]</li><li>分组超时意味着拥塞，分组收到确认则意味着网络未拥塞</li><li>拥塞则少发（拥塞窗口减小），没拥塞则多发（拥塞窗口增加）</li><li>在网络未知的情况下拥塞窗口从最小开始</li><li>收到确认拥塞窗口大小增加</li><li>为提高效率，开始窗口增加速度快，到了一定阶段窗口增加速度变慢</li></ul><p><code>举例</code></p><img src="https://img-blog.csdnimg.cn/20200613111041597.png" width="50%"><p><code>总结</code><br><strong>两个阶段</strong></p><ul><li>慢启动阶段 - - - 乘法增</li><li>拥塞避免阶段 - - - 加法增</li></ul><p><strong>一个阈值</strong></p><ul><li>定义了慢启动阶段和拥塞避免阶段的分界点</li></ul><p><strong>超时发生时</strong></p><ul><li>阈值变成超时的窗口大小的一半</li><li>回到慢启动</li></ul><p><strong>快恢复算法</strong></p><ul><li>当发送端收到连续3个重复的确认时，就执行“乘法减小”算法，把慢开始门限ssthresh减半，直接进入拥塞避免阶段</li></ul><img src="https://img-blog.csdnimg.cn/20200613111515917.png" width="50%"><p><strong>LAND攻击</strong><br><code>攻击方法及原理</code></p><ul><li>方法：将TCP包的源地址和目的地址，源端口和目的端口都设置成相同，导致对方死机</li><li>原理：TCP连接管理的实现存在漏洞</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;拥塞：&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;在某段时间，若对网络中某资源的需求超过了该资源所能提供的可用部分，网络的性能就要变坏，产生拥塞&lt;/li&gt;
&lt;li&gt;若网络中有许多资源同时产生拥塞，网络的性能就要明显变坏，整个网络的吞吐量将随输入负荷的增大而下降&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;拥塞控制：&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;保证网络能够承受现有的网络负荷&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;开环控制和闭环控制&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;code&gt;开环控制&lt;/code&gt;&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;在设计网络时事先将有关发生的拥塞的因素考虑周到，力求网络在工作时不产生拥塞&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
    
      <category term="计算机网络" scheme="http://1979F.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>TCP协议详解</title>
    <link href="http://1979F.github.io/2020/06/11/TCP%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/"/>
    <id>http://1979F.github.io/2020/06/11/TCP协议详解/</id>
    <published>2020-06-11T07:39:02.000Z</published>
    <updated>2020-11-22T07:01:41.940Z</updated>
    
    <content type="html"><![CDATA[<h5 id="一、TCP的设计理念"><a href="#一、TCP的设计理念" class="headerlink" title="一、TCP的设计理念"></a>一、TCP的设计理念</h5><table><thead><tr><th>传输层协议</th><th>端到端控制，实现分用和复用</th></tr></thead><tbody><tr><td>可靠传输机制</td><td>保证端到端数据按序正确的到达、序号机制、确认机制、差错检测机制、缓存机制、重传机制、滑动窗口机制</td></tr><tr><td>其他机制</td><td>流量控制机制、拥塞控制机制</td></tr></tbody></table><p><strong>TCP协议的可靠传输机制</strong></p><img src="https://img-blog.csdnimg.cn/2020061023181263.png" width="50%"><p><strong>TCP的首部</strong></p><img src="https://img-blog.csdnimg.cn/20200611084558107.png" width="50%"><ul><li><code>源端口和目的端口</code>字段——各占2字节。端口是传输层与应用层的服务接口，类似一个地址标识。传输层的复用和分用功能都要通过端口才能实现</li><li><code>序号</code>字段——占4字节。TCP连接中传送的数据流中的每一个字节都编上一个号。序号字段的值指的是本报文段所发送的数据的第一个字节的编号</li><li><code>确认号</code>字段——占4字节，是期望收到对方的下一个报文段的数据的第一个字节的序号。注意，当有数据要发送给对方时，顺便确认，当没有数据发送给对方时，单独发一个确认报文</li><li>特殊标记（Flag）：每个标记占一个bit，有特殊约定</li><li><code>URG</code>——紧急比特标记，当URG置为1时，表明紧急指针字段有效。通知本报文段中有紧急数据，应尽快传送，紧急数据的优先级要高。</li><li><code>ACK</code>——只有当ACK置为1时，确认号字段才有效。正常情况下只有第一次握手时ACK=0</li><li><code>PSH</code>（Push）——推送比特，接收方收到推送比特置为1的报文段，就尽快地将该报文段的数据交付给接收应用进程，而不再等到整个缓存都填满了后再向上交付</li><li><code>RST</code>（ReSet）——复位比特，当RST=1时，表明TCP连接中出现严重差错，必须强行释放连接，属于单方面强行断开连接</li><li><code>SYN</code>——同步比特，SYN置为1，表示这是一个连接请求报文。正常情况下只有第一次握手和第二次握手时SYN等于1，其余都等于0</li><li><code>FIN</code>（Final）——终止比特，用来正常释放一个连接。当FIN=1时，表明此报文段的发送端的数据已发送完毕，并请求对方释放连接，当对方确认后，会释放发送缓存</li><li><code>窗口</code>字段——占2字节。窗口字段是<code>流量控制</code>的关键，用来控制对方发送窗口的大小，单位为字节。接收方根据自身的缓存大小确定自己的接收窗口大小，然后通知对方以确定对方的发送窗口的上限</li><li>检验和——占2字节。检验和字段检验的范围包括首部和数据这两部分，在计算检验和时，要在TCP报文段的前面加上12字节的伪首部</li><li><code>紧急指针</code>字段——占16bit，紧急指针指出在本报文段中的紧急数据的最后一个字节的序号 </li></ul><a id="more"></a><p><strong>TCP的确认机制</strong><br><code>序号</code></p><ul><li>按字节编号</li></ul><p><code>确认</code></p><ul><li>期望确认：期待对方发送的下一个报文的序号</li><li>累积确认：收到某个分组的确认意味着该分组及之前所有分组都正确收到</li></ul><p><code>特殊报文的确认问题</code></p><ul><li>对于没有包含数据的确认报文段不再确认</li><li>对于含<code>特殊标记的数据段，即使没有任何的数据接收，确认号也要加1</code></li></ul><p><strong>TCP 建立连接的作用</strong><br><code>作用：</code></p><ul><li>使每一方能够确知对方的存在</li><li>允许双方协议一些参数（如最大报文段长度，最大窗口大小，服务质量等）</li><li>对传输实体资源进行分配</li></ul><p><strong>TCP的三次握手建立连接</strong></p><img src="https://img-blog.csdnimg.cn/2020061111395021.png" width="50%"><p>注意：这个时候如果客户端再发请求，序号还是从x+1开始</p><p><code>注意：</code></p><ul><li>建立连接时会初始化相关参数，分配缓存等资源</li><li>服务端收到第一次握手后默认会跟踪该连接75秒</li></ul><img src="https://img-blog.csdnimg.cn/20200611142439702.png" width="50%"><p><strong>SYN Flooding攻击</strong></p><ul><li>攻击原理：服务器进行第二次握手之后会在超时时间内（一般为75秒）跟踪该连接，未收到第三次握手会不断重发，消耗资源</li></ul><img src="https://img-blog.csdnimg.cn/20200611142812953.png" width="50%"><p><strong>攻击防范</strong></p><ul><li>缩短TCP超时时间</li><li>更改TCP的搬开连接数</li><li>TCP-z，监控TCP状态</li><li>通过防火墙、路由器等过滤网关防护</li><li>使用SYN Cookie技术</li></ul><p><strong>TCP的四次挥手断开连接</strong><br><img src="https://img-blog.csdnimg.cn/20200611143808219.png" width="45%"><img src="https://img-blog.csdnimg.cn/20200611144034408.png" width="45%"></p><p><strong>TCP的时间等待计时器</strong></p><ul><li>防止最后一个ACK丢失导致断开连接出现异常</li><li>一般等于二倍报文段寿命长度</li></ul><img src="https://img-blog.csdnimg.cn/20200611150001596.png" width="30%"><p><strong>保活计时器</strong></p><ul><li>防止两个TCP之间的连接长时间的空闲</li></ul><p><strong>TCP的重传机制</strong></p><p><code>选择性重传机制</code></p><ul><li>当数据超时则需要重传，需要重传定时器</li><li>TCP属于使用累积确认的选择性重传协议</li></ul><p><code>重传超时时间的确定</code></p><img src="https://img-blog.csdnimg.cn/20200611153222785.png" width="50%"><p><code>快重传机制</code></p><ul><li>当连续收到三个重复的确认，直接重传所需分组，提高效率</li></ul><p><strong>TCP的流量控制</strong><br><code>接收方</code>：明确地通过TCP首部的窗口字段发送接收窗口大小，从而限制发送方发送窗口的最大值<br><code>发送方</code>：保证发送窗口大小不超过对方发送的接收窗口的大小</p>]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;一、TCP的设计理念&quot;&gt;&lt;a href=&quot;#一、TCP的设计理念&quot; class=&quot;headerlink&quot; title=&quot;一、TCP的设计理念&quot;&gt;&lt;/a&gt;一、TCP的设计理念&lt;/h5&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;传输层协议&lt;/th&gt;
&lt;th&gt;端到端控制，实现分用和复用&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;可靠传输机制&lt;/td&gt;
&lt;td&gt;保证端到端数据按序正确的到达、序号机制、确认机制、差错检测机制、缓存机制、重传机制、滑动窗口机制&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;其他机制&lt;/td&gt;
&lt;td&gt;流量控制机制、拥塞控制机制&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;strong&gt;TCP协议的可靠传输机制&lt;/strong&gt;&lt;/p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/2020061023181263.png&quot; width=&quot;50%&quot;&gt;&lt;p&gt;&lt;strong&gt;TCP的首部&lt;/strong&gt;&lt;/p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20200611084558107.png&quot; width=&quot;50%&quot;&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;源端口和目的端口&lt;/code&gt;字段——各占2字节。端口是传输层与应用层的服务接口，类似一个地址标识。传输层的复用和分用功能都要通过端口才能实现&lt;/li&gt;
&lt;li&gt;&lt;code&gt;序号&lt;/code&gt;字段——占4字节。TCP连接中传送的数据流中的每一个字节都编上一个号。序号字段的值指的是本报文段所发送的数据的第一个字节的编号&lt;/li&gt;
&lt;li&gt;&lt;code&gt;确认号&lt;/code&gt;字段——占4字节，是期望收到对方的下一个报文段的数据的第一个字节的序号。注意，当有数据要发送给对方时，顺便确认，当没有数据发送给对方时，单独发一个确认报文&lt;/li&gt;
&lt;li&gt;特殊标记（Flag）：每个标记占一个bit，有特殊约定&lt;/li&gt;
&lt;li&gt;&lt;code&gt;URG&lt;/code&gt;——紧急比特标记，当URG置为1时，表明紧急指针字段有效。通知本报文段中有紧急数据，应尽快传送，紧急数据的优先级要高。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ACK&lt;/code&gt;——只有当ACK置为1时，确认号字段才有效。正常情况下只有第一次握手时ACK=0&lt;/li&gt;
&lt;li&gt;&lt;code&gt;PSH&lt;/code&gt;（Push）——推送比特，接收方收到推送比特置为1的报文段，就尽快地将该报文段的数据交付给接收应用进程，而不再等到整个缓存都填满了后再向上交付&lt;/li&gt;
&lt;li&gt;&lt;code&gt;RST&lt;/code&gt;（ReSet）——复位比特，当RST=1时，表明TCP连接中出现严重差错，必须强行释放连接，属于单方面强行断开连接&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SYN&lt;/code&gt;——同步比特，SYN置为1，表示这是一个连接请求报文。正常情况下只有第一次握手和第二次握手时SYN等于1，其余都等于0&lt;/li&gt;
&lt;li&gt;&lt;code&gt;FIN&lt;/code&gt;（Final）——终止比特，用来正常释放一个连接。当FIN=1时，表明此报文段的发送端的数据已发送完毕，并请求对方释放连接，当对方确认后，会释放发送缓存&lt;/li&gt;
&lt;li&gt;&lt;code&gt;窗口&lt;/code&gt;字段——占2字节。窗口字段是&lt;code&gt;流量控制&lt;/code&gt;的关键，用来控制对方发送窗口的大小，单位为字节。接收方根据自身的缓存大小确定自己的接收窗口大小，然后通知对方以确定对方的发送窗口的上限&lt;/li&gt;
&lt;li&gt;检验和——占2字节。检验和字段检验的范围包括首部和数据这两部分，在计算检验和时，要在TCP报文段的前面加上12字节的伪首部&lt;/li&gt;
&lt;li&gt;&lt;code&gt;紧急指针&lt;/code&gt;字段——占16bit，紧急指针指出在本报文段中的紧急数据的最后一个字节的序号 &lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
    
      <category term="计算机网络" scheme="http://1979F.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>滑动窗口协议</title>
    <link href="http://1979F.github.io/2020/06/10/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E5%8D%8F%E8%AE%AE/"/>
    <id>http://1979F.github.io/2020/06/10/滑动窗口协议/</id>
    <published>2020-06-10T15:10:42.000Z</published>
    <updated>2020-11-22T07:30:32.766Z</updated>
    
    <content type="html"><![CDATA[<h5 id="滑动窗口协议"><a href="#滑动窗口协议" class="headerlink" title="滑动窗口协议"></a>滑动窗口协议</h5><img src="https://img-blog.csdnimg.cn/20200610212224143.png" width="50%"><p><strong>核心思想：</strong></p><ul><li>发送方在没有收到对方的ACK的时候可以发送多个数据包</li></ul><p><strong>特性</strong></p><ul><li>发送方使用<code>发送窗口</code>限制没收到确认时允许发送的数据量</li><li>必须<code>增加序号的个数</code>，发送方和接收方需要增加缓存</li><li>常见的两种滑动窗口协议：<code>GBN（回退N步）和SR（选择性重传）</code><img src="https://img-blog.csdnimg.cn/20200610212709893.png" width="50%"></li></ul><p><strong>GBN的工作方式</strong></p><ul><li>发送方：窗口不满则发送至窗口满，窗口满则等待，收到确认窗口向后移动，<code>某个分组出错或丢失则重传该分组及其后面所有已发送但未被确认的分组</code></li><li>接收方：<code>对按序正确到达的分组确认，乱序或错误的分组丢弃且发送最后一次正确收到的分组的确认</code></li></ul><a id="more"></a><p><strong>累计确认机制</strong></p><ul><li>发送方收到某个分组的确认意味着该分组及之前所有分组接收方都正确收到</li></ul><p><strong>GBN协议演示</strong><br><img src="https://img-blog.csdnimg.cn/20200610212912234.png" width="50%"></p><p><strong>SR的工作方式</strong><br>SR（selective repeat）选择性重传</p><ul><li>发送方<code>某个分组出错或丢失只重传该分组</code></li><li>接收方<code>增加接收窗口（接收缓存）</code>，若收到的分组<code>在接受窗口内且乱序，缓存该分组</code>，等到分组按序后一起提交，接收窗口的大小一般等于发送方发送窗口的大小</li><li>也是累计确认</li></ul><p><strong>SR的演示</strong></p><img src="https://img-blog.csdnimg.cn/20200610223252831.png" width="50%"><p><strong>窗口大小和序号的关系</strong></p><ul><li>GBN窗口的最大值等于序号的个数-1</li><li>SR窗口的最大值等于序号的一半</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;滑动窗口协议&quot;&gt;&lt;a href=&quot;#滑动窗口协议&quot; class=&quot;headerlink&quot; title=&quot;滑动窗口协议&quot;&gt;&lt;/a&gt;滑动窗口协议&lt;/h5&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20200610212224143.png&quot; width=&quot;50%&quot;&gt;&lt;p&gt;&lt;strong&gt;核心思想：&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;发送方在没有收到对方的ACK的时候可以发送多个数据包&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;特性&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;发送方使用&lt;code&gt;发送窗口&lt;/code&gt;限制没收到确认时允许发送的数据量&lt;/li&gt;
&lt;li&gt;必须&lt;code&gt;增加序号的个数&lt;/code&gt;，发送方和接收方需要增加缓存&lt;/li&gt;
&lt;li&gt;常见的两种滑动窗口协议：&lt;code&gt;GBN（回退N步）和SR（选择性重传）&lt;/code&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20200610212709893.png&quot; width=&quot;50%&quot;&gt;

&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;GBN的工作方式&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;发送方：窗口不满则发送至窗口满，窗口满则等待，收到确认窗口向后移动，&lt;code&gt;某个分组出错或丢失则重传该分组及其后面所有已发送但未被确认的分组&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;接收方：&lt;code&gt;对按序正确到达的分组确认，乱序或错误的分组丢弃且发送最后一次正确收到的分组的确认&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
    
      <category term="计算机网络" scheme="http://1979F.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>FTP协议、电子邮件系统与Telnet远程控制</title>
    <link href="http://1979F.github.io/2020/06/08/FTP%E5%8D%8F%E8%AE%AE%E3%80%81%E7%94%B5%E5%AD%90%E9%82%AE%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%B8%8ETelnet%E8%BF%9C%E7%A8%8B%E6%8E%A7%E5%88%B6/"/>
    <id>http://1979F.github.io/2020/06/08/FTP协议、电子邮件系统与Telnet远程控制/</id>
    <published>2020-06-08T02:01:54.000Z</published>
    <updated>2020-11-22T07:18:15.576Z</updated>
    
    <content type="html"><![CDATA[<h5 id="一、FTP协议概述"><a href="#一、FTP协议概述" class="headerlink" title="一、FTP协议概述"></a>一、FTP协议概述</h5><p><strong>FTP定义</strong></p><ul><li>文件传输协议</li><li>用于因特网文件传输</li></ul><p><strong>FTP特性</strong></p><ul><li>使用客户端/服务器模式</li><li>使用TCP提供可靠的传输</li><li>FTP属于维护状态的协议</li><li>FTP使用<code>两条TCP连接</code>完成数据传输</li></ul><p><strong>FTP的两条连接</strong></p><img src="https://img-blog.csdnimg.cn/20200608092104764.png" width="60%"><p><code>数据连接问题</code></p><ul><li>当有具体文件或目录内容传输时，临时建立数据连接</li><li>主动模式下由服务器方发起，服务器端口号20</li><li>被动模式下由客户端发起，服务器端口号不确定</li></ul><a id="more"></a><h5 id="二、电子邮件系统的组成"><a href="#二、电子邮件系统的组成" class="headerlink" title="二、电子邮件系统的组成"></a>二、电子邮件系统的组成</h5><p><strong>电子邮件系统的组成</strong><br><code>用户代理</code></p><ul><li>电子邮件客户端软件，如浏览器，Outlook，Outlook Express，Foxmail等</li></ul><p><code>邮件服务器</code></p><ul><li>提供邮件服务的主机，如sina邮件服务器sina.com，google的邮件服务器gmail.com</li></ul><p><code>邮件发送和接收协议</code></p><ul><li>HTTP，SMTP，POP，IMAP等<img src="https://img-blog.csdnimg.cn/20200608093026432.png" width="60%"></li></ul><p><strong>SMTP协议</strong><br><code>SMTP协议定义</code></p><ul><li>Simple Mail Transfer Protocol：简单邮件传输协议</li><li>用于使用邮件代理发给邮件服务器或邮件服务器之间转发邮件</li></ul><p><code>SMTP特性</code></p><ul><li>使用TCP可靠的传送邮件报文，服务器监听端口号为25</li><li>使用命令/响应代码完成邮件传输的控制交互</li><li>SMTP只能传送7bit的ASCII码的邮件报文</li><li>SMTP采用持续连接的方式发邮件</li></ul><p><code>问题1：SMTP无认证</code></p><ul><li>将导致大量的垃圾邮件</li><li>ESMTP：增强型SMTP，发邮件需要用户名密码验证</li></ul><p><code>问题2：SMTP只能传输ASCII码的文本文件</code></p><ul><li>使用MIME多目标邮件拓展协议</li><li>MIME使用BASE64编码或QP编码将非ASCII码转为ASCII码</li></ul><p><strong>邮件接收协议简介</strong></p><p><code>用户收取邮件常用的协议</code></p><ul><li>POP（Post Office Protocol邮局协议）：服务器端口默认110</li><li>IMAP（Internet Mail Access Protocol）【因特网邮件访问协议】：服务器默认端口143</li><li>HTTP</li></ul><p><code>POP3协议</code><br><img src="https://img-blog.csdnimg.cn/20200608095407547.png" width="60%"></p><p><code>IMAP协议</code></p><ul><li>IMAP具备和POP一样的邮件下载功能</li><li>IMAP允许只读取邮件中的某一个部分</li><li>IMAP提供操作的三种模式<br>1、在线方式：邮件保留在Mail服务器端，客户端可以对其进行管理。其使用方式与WebMail相类似<br>2、离线方式：邮件保留在客户端，客户端可以对其进行管理<br>3、分离方式：邮件的一部分在Mail服务器端，一部分在客户端</li></ul><h5 id="三、Telnet远程控制"><a href="#三、Telnet远程控制" class="headerlink" title="三、Telnet远程控制"></a>三、Telnet远程控制</h5><ul><li>Telnet用于终端使用命令行方式对服务器进行远程控制。端口号为TCP的23</li><li>Telnet的客户端称为VTY（虚拟终端），<code>Telnet不安全</code>，明文传输，可以用更安全的SSH协议替代</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;一、FTP协议概述&quot;&gt;&lt;a href=&quot;#一、FTP协议概述&quot; class=&quot;headerlink&quot; title=&quot;一、FTP协议概述&quot;&gt;&lt;/a&gt;一、FTP协议概述&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;FTP定义&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;文件传输协议&lt;/li&gt;
&lt;li&gt;用于因特网文件传输&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;FTP特性&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;使用客户端/服务器模式&lt;/li&gt;
&lt;li&gt;使用TCP提供可靠的传输&lt;/li&gt;
&lt;li&gt;FTP属于维护状态的协议&lt;/li&gt;
&lt;li&gt;FTP使用&lt;code&gt;两条TCP连接&lt;/code&gt;完成数据传输&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;FTP的两条连接&lt;/strong&gt;&lt;/p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20200608092104764.png&quot; width=&quot;60%&quot;&gt;&lt;p&gt;&lt;code&gt;数据连接问题&lt;/code&gt;&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;当有具体文件或目录内容传输时，临时建立数据连接&lt;/li&gt;
&lt;li&gt;主动模式下由服务器方发起，服务器端口号20&lt;/li&gt;
&lt;li&gt;被动模式下由客户端发起，服务器端口号不确定&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
    
      <category term="计算机网络" scheme="http://1979F.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>HTTP报文的格式</title>
    <link href="http://1979F.github.io/2020/06/08/HTTP%E6%8A%A5%E6%96%87%E7%9A%84%E6%A0%BC%E5%BC%8F/"/>
    <id>http://1979F.github.io/2020/06/08/HTTP报文的格式/</id>
    <published>2020-06-08T00:59:16.000Z</published>
    <updated>2020-11-22T07:16:03.771Z</updated>
    
    <content type="html"><![CDATA[<h5 id="一、HTTP请求报文的格式"><a href="#一、HTTP请求报文的格式" class="headerlink" title="一、HTTP请求报文的格式"></a>一、HTTP请求报文的格式</h5><img src="https://img-blog.csdnimg.cn/20200607224516138.png" width="60%"><ul><li>两个回车换行表示首部的结尾</li><li>注意：HTTP协议首部使用ASCII码作为编码方式</li><li>HTTP请求报文提交表单时会包含数据</li></ul><h5 id="二、HTTP响应报文格式"><a href="#二、HTTP响应报文格式" class="headerlink" title="二、HTTP响应报文格式"></a>二、HTTP响应报文格式</h5><img src="https://img-blog.csdnimg.cn/20200607230457258.png" width="60%">]]></content>
    
    <summary type="html">
    
      
      
        &lt;h5 id=&quot;一、HTTP请求报文的格式&quot;&gt;&lt;a href=&quot;#一、HTTP请求报文的格式&quot; class=&quot;headerlink&quot; title=&quot;一、HTTP请求报文的格式&quot;&gt;&lt;/a&gt;一、HTTP请求报文的格式&lt;/h5&gt;&lt;img src=&quot;https://img-blog.cs
      
    
    </summary>
    
    
    
      <category term="计算机网络" scheme="http://1979F.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>www万维网和HTTP协议</title>
    <link href="http://1979F.github.io/2020/06/06/www%E4%B8%87%E7%BB%B4%E7%BD%91%E5%92%8CHTTP%E5%8D%8F%E8%AE%AE/"/>
    <id>http://1979F.github.io/2020/06/06/www万维网和HTTP协议/</id>
    <published>2020-06-06T02:29:07.000Z</published>
    <updated>2020-11-22T06:58:55.010Z</updated>
    
    <content type="html"><![CDATA[<h5 id="万维网的相关概念"><a href="#万维网的相关概念" class="headerlink" title="万维网的相关概念"></a>万维网的相关概念</h5><p><strong>万维网的客户端程序</strong></p><ul><li>浏览器：IE、firefox、chrome</li></ul><p><strong>万维网的服务器端软件</strong></p><ul><li>IIS，Tomcat、Apache </li></ul><p><strong>万维网的模式</strong></p><ul><li>采用C/S模式</li></ul><p><strong>Web页面</strong></p><ul><li>由文字、图片、声音、视频等多种对象组成</li></ul><p><strong>HTTP协议</strong></p><ul><li>Web页面传输方式</li></ul><p><code>如何标志分布在因特网上的万维网文档？</code></p><ul><li>使用整个因特网范围内唯一的统一资源定位符URL（Uniform Resource Locator）来标识万维网上的各种文档</li><li><code>&lt;URL的访问方式&gt;：// &lt;主机&gt;：&lt;端口&gt;/&lt;路径&gt;</code></li></ul><a id="more"></a><p><code>&lt;URL的访问方式&gt;</code><br>ftp：文件传送协议<br>http：超文本传送协议<br>https：基于安全的套接层的http协议</p><p><code>&lt;主机&gt;</code>：存放资源的主机在因特网中的域名或IP地址<br><code>&lt;端口&gt;</code>：根据协议有默认值，如http为80端口，https为443，默认端口可以省略<br><code>&lt;路径&gt;</code>：文件在主机中的相对位置，当路径省略时自动打开默认文档</p><h5 id="HTTP协议概述"><a href="#HTTP协议概述" class="headerlink" title="HTTP协议概述"></a>HTTP协议概述</h5><p><strong>定义</strong></p><ul><li><code>超文本</code>传输（hypertext transfer protocol）协议。</li><li>定义WEB页面在因特网上的交互方式的应用层协议。</li></ul><p><code>WEB页面传输需要可靠传输吗？</code><br>需要，使用TCP协议作为传输层，服务器默认端口号为80</p><p><code>页面文本和链接的对象怎么传输？</code></p><ul><li><code>HTTP1.0</code>：RFC 1945定义</li><li><code>HTTP1.1</code>：RFC 2068定义</li></ul><p><strong>HTTP1.0默认特性</strong><br><img src="https://img-blog.csdnimg.cn/20200606101614332.png" width="50%"><br><strong>非持续连接</strong>：传完一个对象就断开连接，获得对象至少需要2 RTTS（建立连接和获取对象），每次传送都要受到TCP初始化时慢启动影响<br><strong>非流水线作业方式</strong></p><img src="https://img-blog.csdnimg.cn/20200606102320446.png" width="20%"><p><strong>HTTP1.1的默认特性</strong><br><img src="https://img-blog.csdnimg.cn/20200606102022101.png" width="50%"><br><strong>持续连接</strong>：一条TCP连接传多个对象<br><strong>流水线作业方式</strong><br><img src="https://img-blog.csdnimg.cn/20200606102214674.png" width="20%"></p><p><strong>HTTP协议首部的格式</strong></p><ul><li>HTTP请求报文</li><li>HTTP响应报文</li></ul><p><strong>HTTP协议的设计原则</strong></p><ul><li>满足协议的功能</li><li>注意协议的拓展性</li><li>便于程序处理</li><li>注意协议的通信效率</li><li>其他细节的处理</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;万维网的相关概念&quot;&gt;&lt;a href=&quot;#万维网的相关概念&quot; class=&quot;headerlink&quot; title=&quot;万维网的相关概念&quot;&gt;&lt;/a&gt;万维网的相关概念&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;万维网的客户端程序&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;浏览器：IE、firefox、chrome&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;万维网的服务器端软件&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;IIS，Tomcat、Apache &lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;万维网的模式&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;采用C/S模式&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Web页面&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;由文字、图片、声音、视频等多种对象组成&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;HTTP协议&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;Web页面传输方式&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;code&gt;如何标志分布在因特网上的万维网文档？&lt;/code&gt;&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;使用整个因特网范围内唯一的统一资源定位符URL（Uniform Resource Locator）来标识万维网上的各种文档&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;URL的访问方式&amp;gt;：// &amp;lt;主机&amp;gt;：&amp;lt;端口&amp;gt;/&amp;lt;路径&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
    
      <category term="计算机网络" scheme="http://1979F.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
</feed>
